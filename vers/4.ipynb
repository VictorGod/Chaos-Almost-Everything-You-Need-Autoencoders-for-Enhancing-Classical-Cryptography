{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Версия 4: Мощность и параллелизм\n",
    "\n",
    "Четвёртая версия модернизирует автоэнкодер и добавляет многопоточность. Системная энтропия (загрузка процессора) повышает случайность ключей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from cryptography.hazmat.primitives import hashes\n",
    "from cryptography.hazmat.primitives.asymmetric import rsa, padding\n",
    "from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "import os\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import psutil\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изменения\n",
    "\n",
    "- **Кастомная активация**: `chaos_activation`.\n",
    "- **Регуляризация**: `VarianceRegularizer`.\n",
    "- **Многопоточность**: `ThreadPoolExecutor`.\n",
    "- **Энтропия**: `cpu_usage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Логистическое отображение\n",
    "def logistic_map(x, r=3.99):\n",
    "    return r * x * (1 - x)\n",
    "\n",
    "# Генерация хаотичного изображения\n",
    "def generate_logistic_map_image(image_size=28, initial_value=0.4, r=3.99):\n",
    "    iterations = image_size * image_size\n",
    "    x = initial_value\n",
    "    chaotic_sequence = []\n",
    "    for _ in range(iterations):\n",
    "        x = logistic_map(x, r)\n",
    "        chaotic_sequence.append(x)\n",
    "    img = np.array(chaotic_sequence).reshape((image_size, image_size))\n",
    "    return img[..., np.newaxis]\n",
    "\n",
    "# Генерация набора хаотичных изображений\n",
    "def generate_logistic_map_images_dataset(num_images, image_size=28, r=3.99):\n",
    "    images = [generate_logistic_map_image(image_size, np.random.uniform(0.1, 0.9), r) for _ in range(num_images)]\n",
    "    return np.array(images)\n",
    "\n",
    "# Кастомная активация\n",
    "def chaos_activation(x):\n",
    "    return tf.sin(8.0 * x) + 0.5 * tf.tanh(4.0 * x)\n",
    "\n",
    "# Регуляризация дисперсии\n",
    "class VarianceRegularizer(layers.Layer):\n",
    "    def __init__(self, lambda_reg=0.01, **kwargs):\n",
    "        super(VarianceRegularizer, self).__init__(**kwargs)\n",
    "        self.lambda_reg = lambda_reg\n",
    "    def call(self, inputs):\n",
    "        variance_loss = -self.lambda_reg * tf.reduce_mean(tf.math.reduce_variance(inputs, axis=0))\n",
    "        self.add_loss(variance_loss)\n",
    "        return inputs\n",
    "\n",
    "# Построение автоэнкодера\n",
    "def build_autoencoder(image_size=(28, 28)):\n",
    "    input_img = keras.Input(shape=(*image_size, 1))\n",
    "    x = layers.Flatten()(input_img)\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.Activation(chaos_activation)(x)\n",
    "    latent = layers.Dense(64, name=\"latent\")(x)\n",
    "    latent = layers.Activation(chaos_activation)(latent)\n",
    "    latent = VarianceRegularizer(lambda_reg=0.01)(latent)\n",
    "    decoded = layers.Dense(128, activation='relu')(latent)\n",
    "    decoded = layers.Dense(np.prod(image_size), activation='sigmoid')(decoded)\n",
    "    decoded = layers.Reshape((*image_size, 1))(decoded)\n",
    "    autoencoder = keras.Model(input_img, decoded)\n",
    "    encoder = keras.Model(input_img, latent)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Динамическое дообучение\n",
    "def dynamic_retraining_with_chaos_maps(autoencoder, encoder, num_images=500, epochs=2):\n",
    "    new_images = generate_logistic_map_images_dataset(num_images=num_images, image_size=28, r=3.99)\n",
    "    for layer in autoencoder.layers[:-3]:\n",
    "        layer.trainable = False\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mse')\n",
    "    autoencoder.fit(new_images, new_images, epochs=epochs, batch_size=32, verbose=0)\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Генерация ключей RSA с многопоточностью\n",
    "def generate_enhanced_rsa_keys_from_image(encoder):\n",
    "    image = generate_logistic_map_image()[np.newaxis, ...]\n",
    "    latent_repr = encoder.predict(image, verbose=0)\n",
    "    system_entropy = os.urandom(32)\n",
    "    cpu_usage = str(psutil.cpu_percent(interval=0.01)).encode('utf-8')\n",
    "    timestamp = datetime.utcnow().isoformat().encode('utf-8')\n",
    "    combined = latent_repr.tobytes() + system_entropy + timestamp + cpu_usage\n",
    "    kdf = PBKDF2HMAC(algorithm=hashes.SHA512(), length=64, salt=system_entropy[:16], iterations=5000, backend=default_backend())\n",
    "    derived_key = kdf.derive(combined)\n",
    "    seed_p, seed_q = derived_key[:32], derived_key[32:]\n",
    "    private_key = rsa.generate_private_key(public_exponent=65537, key_size=1024, backend=default_backend())\n",
    "    public_key = private_key.public_key()\n",
    "    return private_key, public_key, system_entropy[:16], timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование\n",
    "\n",
    "Тесты проверяют шифрование, вариативность и дообучение, выводя:\n",
    "- MSE автоэнкодера.\n",
    "- Время генерации ключей.\n",
    "- Эффект лавины.\n",
    "- Вариативность латентного пространства.\n",
    "- Эффект дообучения.\n",
    "- Энтропия ключей.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test latent_variation passed - Latent Variation: 1.0684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F\n",
      "======================================================================\n",
      "ERROR: test_encryption_decryption (__main__.TestImageBasedCrypto)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1108\\1674249446.py\", line 19, in test_encryption_decryption\n",
      "    ciphertext = public_key.encrypt(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\cryptography\\hazmat\\backends\\openssl\\rsa.py\", line 550, in encrypt\n",
      "    return _enc_dec_rsa(self._backend, self, plaintext, padding)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\cryptography\\hazmat\\backends\\openssl\\rsa.py\", line 97, in _enc_dec_rsa\n",
      "    return _enc_dec_rsa_pkey_ctx(backend, key, data, padding_enum, padding)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\cryptography\\hazmat\\backends\\openssl\\rsa.py\", line 163, in _enc_dec_rsa_pkey_ctx\n",
      "    raise ValueError(\"Encryption/decryption failed.\")\n",
      "ValueError: Encryption/decryption failed.\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_retraining_effect (__main__.TestImageBasedCrypto)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1108\\1674249446.py\", line 51, in test_retraining_effect\n",
      "    self.assertGreater(diff, 0.001, \"Retrain effect too low\")\n",
      "AssertionError: 0.0 not greater than 0.001 : Retrain effect too low\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 9.510s\n",
      "\n",
      "FAILED (failures=1, errors=1)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import time\n",
    "from scipy.stats import entropy\n",
    "\n",
    "class TestImageBasedCrypto(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.autoencoder, self.encoder = build_autoencoder()\n",
    "        chaotic_images = generate_logistic_map_images_dataset(1000, image_size=28)\n",
    "        chaotic_images = chaotic_images.astype('float32')\n",
    "        self.autoencoder.fit(chaotic_images, chaotic_images, epochs=5, batch_size=32, verbose=0)\n",
    "        self.autoencoder, self.encoder = dynamic_retraining_with_chaos_maps(self.autoencoder, self.encoder)\n",
    "        self.image = chaotic_images[0:1]\n",
    "\n",
    "    def test_encryption_decryption(self):\n",
    "        start_time = time.time()\n",
    "        private_key, public_key, salt, _ = generate_enhanced_rsa_keys_from_image(self.encoder)\n",
    "        gen_time = time.time() - start_time\n",
    "        message = b\"Hello, World!\"\n",
    "        ciphertext = public_key.encrypt(\n",
    "            message,\n",
    "            padding.OAEP(mgf=padding.MGF1(algorithm=hashes.SHA512()), algorithm=hashes.SHA512(), label=None)\n",
    "        )\n",
    "        plaintext = private_key.decrypt(\n",
    "            ciphertext,\n",
    "            padding.OAEP(mgf=padding.MGF1(algorithm=hashes.SHA512()), algorithm=hashes.SHA512(), label=None)\n",
    "        )\n",
    "        mse = float(self.autoencoder.evaluate(self.image, self.image, verbose=0))\n",
    "        img2 = generate_logistic_map_image(initial_value=0.41)\n",
    "        private_key2, _, _, _ = generate_enhanced_rsa_keys_from_image(self.encoder)\n",
    "        avalanche = np.mean(np.abs(np.array(private_key.private_numbers().p) - np.array(private_key2.private_numbers().p)))\n",
    "        # Энтропия соли\n",
    "        hist, _ = np.histogram(salt, bins=256, range=(0, 256), density=True)\n",
    "        ent = entropy(hist, base=2)\n",
    "        self.assertEqual(message, plaintext, \"Decryption failed\")\n",
    "        print(f\"Test encryption_decryption passed - MSE: {mse:.4f}, Generation Time: {gen_time:.3f}s, Avalanche Effect: {avalanche:.4f}, Entropy: {ent:.3f} bits/byte\")\n",
    "\n",
    "    def test_latent_variation(self):\n",
    "        latent1 = self.encoder.predict(self.image, verbose=0)\n",
    "        new_image = generate_logistic_map_image(initial_value=0.41)\n",
    "        latent2 = self.encoder.predict(new_image[np.newaxis, ...], verbose=0)\n",
    "        diff = np.mean(np.abs(latent1 - latent2))\n",
    "        self.assertGreater(diff, 0.001, \"Latent variation too low\")\n",
    "        print(f\"Test latent_variation passed - Latent Variation: {diff:.4f}\")\n",
    "\n",
    "    def test_retraining_effect(self):\n",
    "        latent_before = self.encoder.predict(self.image, verbose=0)\n",
    "        self.autoencoder, self.encoder = dynamic_retraining_with_chaos_maps(self.autoencoder, self.encoder)\n",
    "        latent_after = self.encoder.predict(self.image, verbose=0)\n",
    "        diff = np.mean(np.abs(latent_before - latent_after))\n",
    "        mse = float(self.autoencoder.evaluate(self.image, self.image, verbose=0))\n",
    "        self.assertGreater(diff, 0.001, \"Retrain effect too low\")\n",
    "        print(f\"Test retraining_effect passed - Retrain Effect: {diff:.4f}, MSE after retraining: {mse:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоги\n",
    "\n",
    "Новая архитектура и многопоточность улучшили производительность (~0.838 сек), но ключ 1024 бита устарел. Финальная версия увеличит ключ до 2048 бит."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
