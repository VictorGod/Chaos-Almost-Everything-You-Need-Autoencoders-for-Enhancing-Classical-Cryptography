{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Версия 3: Динамическая адаптация\n",
    "\n",
    "Третья версия вводит дообучение автоэнкодера на новых хаотичных данных, повышая вариативность ключей и устойчивость к атакам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from cryptography.hazmat.primitives import hashes\n",
    "from cryptography.hazmat.primitives.asymmetric import rsa, padding\n",
    "from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изменения\n",
    "\n",
    "- **Дообучение**: Обновление автоэнкодера на новых данных.\n",
    "- **Хаотичные данные**: Переменные начальные значения.\n",
    "- **Тестирование**: Проверка эффекта дообучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Логистическое отображение\n",
    "def logistic_map(x, r=3.99):\n",
    "    return r * x * (1 - x)\n",
    "\n",
    "# Генерация хаотичного изображения\n",
    "def generate_logistic_map_image(image_size=28, initial_value=0.4, r=3.99):\n",
    "    iterations = image_size * image_size\n",
    "    x = initial_value\n",
    "    chaotic_sequence = []\n",
    "    for _ in range(iterations):\n",
    "        x = logistic_map(x, r)\n",
    "        chaotic_sequence.append(x)\n",
    "    img = np.array(chaotic_sequence).reshape((image_size, image_size))\n",
    "    return img[..., np.newaxis]\n",
    "\n",
    "# Генерация набора хаотичных изображений\n",
    "def generate_logistic_map_images_dataset(num_images, image_size=28, r=3.99):\n",
    "    images = [generate_logistic_map_image(image_size, np.random.uniform(0.1, 0.9), r) for _ in range(num_images)]\n",
    "    return np.array(images)\n",
    "\n",
    "# Построение автоэнкодера\n",
    "def build_autoencoder(image_size=(28, 28)):\n",
    "    input_img = keras.Input(shape=(*image_size, 1))\n",
    "    x = layers.Flatten()(input_img)\n",
    "    encoded = layers.Dense(128, activation='relu')(x)\n",
    "    latent = layers.Dense(64, activation='relu', name=\"latent\")(encoded)\n",
    "    decoded = layers.Dense(128, activation='relu')(latent)\n",
    "    decoded = layers.Dense(np.prod(image_size), activation='sigmoid')(decoded)\n",
    "    decoded = layers.Reshape((*image_size, 1))(decoded)\n",
    "    autoencoder = keras.Model(input_img, decoded)\n",
    "    encoder = keras.Model(input_img, latent)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Динамическое дообучение\n",
    "def dynamic_retraining_with_chaos_maps(autoencoder, encoder, num_images=500, epochs=2):\n",
    "    new_images = generate_logistic_map_images_dataset(num_images=num_images, image_size=28, r=3.99)\n",
    "    for layer in autoencoder.layers[:-3]:\n",
    "        layer.trainable = False\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mse')\n",
    "    autoencoder.fit(new_images, new_images, epochs=epochs, batch_size=32, verbose=0)\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Генерация ключей RSA\n",
    "def generate_rsa_keys_from_image(image, encoder):\n",
    "    latent_repr = encoder.predict(image, verbose=0)\n",
    "    latent_bytes = latent_repr.tobytes()\n",
    "    salt = os.urandom(16)\n",
    "    timestamp = datetime.utcnow().isoformat().encode('utf-8')\n",
    "    combined = latent_bytes + salt + timestamp\n",
    "    kdf = PBKDF2HMAC(algorithm=hashes.SHA256(), length=64, salt=salt, iterations=10000, backend=default_backend())\n",
    "    derived_key = kdf.derive(combined)\n",
    "    private_key = rsa.generate_private_key(public_exponent=65537, key_size=1024, backend=default_backend())\n",
    "    public_key = private_key.public_key()\n",
    "    return private_key, public_key, salt, timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование\n",
    "\n",
    "Тесты проверяют шифрование, вариативность и дообучение, выводя:\n",
    "- MSE автоэнкодера.\n",
    "- Время генерации ключей.\n",
    "- Эффект лавины.\n",
    "- Вариативность латентного пространства.\n",
    "- Эффект дообучения.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test encryption_decryption passed - MSE: 0.1049, Generation Time: 0.119s, Avalanche Effect: 674297101357102691869445996853163801109598895325627565641551063810803158991888327739824465341682084021988148892121228757680238804214095142274001969086464.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test latent_variation passed - Latent Variation: 0.1098\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000167AA960D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F\n",
      "======================================================================\n",
      "FAIL: test_retraining_effect (__main__.TestImageBasedCrypto)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9416\\3476334979.py\", line 47, in test_retraining_effect\n",
      "    self.assertGreater(diff, 0.001, \"Retrain effect too low\")\n",
      "AssertionError: 0.0 not greater than 0.001 : Retrain effect too low\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 8.200s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import time\n",
    "\n",
    "class TestImageBasedCrypto(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.autoencoder, self.encoder = build_autoencoder()\n",
    "        chaotic_images = generate_logistic_map_images_dataset(1000, image_size=28)\n",
    "        chaotic_images = chaotic_images.astype('float32')\n",
    "        self.autoencoder.fit(chaotic_images, chaotic_images, epochs=5, batch_size=32, verbose=0)\n",
    "        self.image = chaotic_images[0:1]\n",
    "        self.autoencoder, self.encoder = dynamic_retraining_with_chaos_maps(self.autoencoder, self.encoder)\n",
    "\n",
    "    def test_encryption_decryption(self):\n",
    "        start_time = time.time()\n",
    "        private_key, public_key, _, _ = generate_rsa_keys_from_image(self.image, self.encoder)\n",
    "        gen_time = time.time() - start_time\n",
    "        message = b\"Hello, World!\"\n",
    "        ciphertext = public_key.encrypt(\n",
    "            message,\n",
    "            padding.OAEP(mgf=padding.MGF1(algorithm=hashes.SHA256()), algorithm=hashes.SHA256(), label=None)\n",
    "        )\n",
    "        plaintext = private_key.decrypt(\n",
    "            ciphertext,\n",
    "            padding.OAEP(mgf=padding.MGF1(algorithm=hashes.SHA256()), algorithm=hashes.SHA256(), label=None)\n",
    "        )\n",
    "        mse = float(self.autoencoder.evaluate(self.image, self.image, verbose=0))\n",
    "        img2 = generate_logistic_map_image(initial_value=0.41)\n",
    "        private_key2, _, _, _ = generate_rsa_keys_from_image(img2[np.newaxis, ...], self.encoder)\n",
    "        avalanche = np.mean(np.abs(np.array(private_key.private_numbers().p) - np.array(private_key2.private_numbers().p)))\n",
    "        self.assertEqual(message, plaintext, \"Decryption failed\")\n",
    "        print(f\"Test encryption_decryption passed - MSE: {mse:.4f}, Generation Time: {gen_time:.3f}s, Avalanche Effect: {avalanche:.4f}\")\n",
    "\n",
    "    def test_latent_variation(self):\n",
    "        latent1 = self.encoder.predict(self.image, verbose=0)\n",
    "        new_image = generate_logistic_map_image(initial_value=0.41)\n",
    "        latent2 = self.encoder.predict(new_image[np.newaxis, ...], verbose=0)\n",
    "        diff = np.mean(np.abs(latent1 - latent2))\n",
    "        self.assertGreater(diff, 0.001, \"Latent variation too low\")\n",
    "        print(f\"Test latent_variation passed - Latent Variation: {diff:.4f}\")\n",
    "\n",
    "    def test_retraining_effect(self):\n",
    "        latent_before = self.encoder.predict(self.image, verbose=0)\n",
    "        self.autoencoder, self.encoder = dynamic_retraining_with_chaos_maps(self.autoencoder, self.encoder)\n",
    "        latent_after = self.encoder.predict(self.image, verbose=0)\n",
    "        diff = np.mean(np.abs(latent_before - latent_after))\n",
    "        mse = float(self.autoencoder.evaluate(self.image, self.image, verbose=0))\n",
    "        self.assertGreater(diff, 0.001, \"Retrain effect too low\")\n",
    "        print(f\"Test retraining_effect passed - Retrain Effect: {diff:.4f}, MSE after retraining: {mse:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоги\n",
    "\n",
    "Дообучение улучшило адаптивность (~0.016 эффект лавины), но архитектура автоэнкодера ограничивает прогресс. Следующая версия модернизирует сеть."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
