{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Версия 2: Хаотичность для стойкости\n",
    "\n",
    "Вторая версия заменяет MNIST на хаотичные изображения из логистического отображения, повышая энтропию и вариативность ключей. Это улучшает криптографическую стойкость."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from cryptography.hazmat.primitives import hashes\n",
    "from cryptography.hazmat.primitives.asymmetric import rsa, padding\n",
    "from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изменения\n",
    "\n",
    "- **Хаотичные данные**: Логистическое отображение для генерации изображений.\n",
    "- **Автоэнкодер**: Обучается на хаотичных данных.\n",
    "- **Тестирование**: Проверка вариативности латентного пространства."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Логистическое отображение\n",
    "def logistic_map(x, r=3.99):\n",
    "    return r * x * (1 - x)\n",
    "\n",
    "# Генерация хаотичного изображения\n",
    "def generate_logistic_map_image(image_size=28, initial_value=0.4, r=3.99):\n",
    "    iterations = image_size * image_size\n",
    "    x = initial_value\n",
    "    chaotic_sequence = []\n",
    "    for _ in range(iterations):\n",
    "        x = logistic_map(x, r)\n",
    "        chaotic_sequence.append(x)\n",
    "    img = np.array(chaotic_sequence).reshape((image_size, image_size))\n",
    "    return img[..., np.newaxis]\n",
    "\n",
    "# Построение автоэнкодера\n",
    "def build_autoencoder(image_size=(28, 28)):\n",
    "    input_img = keras.Input(shape=(*image_size, 1))\n",
    "    x = layers.Flatten()(input_img)\n",
    "    encoded = layers.Dense(128, activation='relu')(x)\n",
    "    latent = layers.Dense(64, activation='relu', name=\"latent\")(encoded)\n",
    "    decoded = layers.Dense(128, activation='relu')(latent)\n",
    "    decoded = layers.Dense(np.prod(image_size), activation='sigmoid')(decoded)\n",
    "    decoded = layers.Reshape((*image_size, 1))(decoded)\n",
    "    autoencoder = keras.Model(input_img, decoded)\n",
    "    encoder = keras.Model(input_img, latent)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Генерация ключей RSA\n",
    "def generate_rsa_keys_from_image(image, encoder):\n",
    "    latent_repr = encoder.predict(image, verbose=0)\n",
    "    latent_bytes = latent_repr.tobytes()\n",
    "    salt = os.urandom(16)\n",
    "    timestamp = datetime.utcnow().isoformat().encode('utf-8')\n",
    "    combined = latent_bytes + salt + timestamp\n",
    "    kdf = PBKDF2HMAC(algorithm=hashes.SHA256(), length=64, salt=salt, iterations=10000, backend=default_backend())\n",
    "    derived_key = kdf.derive(combined)\n",
    "    private_key = rsa.generate_private_key(public_exponent=65537, key_size=1024, backend=default_backend())\n",
    "    public_key = private_key.public_key()\n",
    "    return private_key, public_key, salt, timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование\n",
    "\n",
    "Тесты проверяют шифрование и вариативность, выводя:\n",
    "- MSE автоэнкодера.\n",
    "- Время генерации ключей.\n",
    "- Эффект лавины.\n",
    "- Вариативность латентного пространства.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002211DAB0D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test encryption_decryption passed - MSE: 0.0000, Generation Time: 0.198s, Avalanche Effect: 823808139230397267373808092525873703668801732035247049653067676318687482263817787531216574629468826347502334856220735429137874023003379472942135404134400.0000\n",
      "WARNING:tensorflow:6 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002211DAB3880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 3.798s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test latent_variation passed - Latent Variation: 0.1152\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import time\n",
    "\n",
    "class TestImageBasedCrypto(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.autoencoder, self.encoder = build_autoencoder()\n",
    "        chaotic_images = np.array([generate_logistic_map_image() for _ in range(1000)])\n",
    "        chaotic_images = chaotic_images.astype('float32')\n",
    "        self.autoencoder.fit(chaotic_images, chaotic_images, epochs=5, batch_size=32, verbose=0)\n",
    "        self.image = chaotic_images[0:1]\n",
    "\n",
    "    def test_encryption_decryption(self):\n",
    "        start_time = time.time()\n",
    "        private_key, public_key, _, _ = generate_rsa_keys_from_image(self.image, self.encoder)\n",
    "        gen_time = time.time() - start_time\n",
    "        message = b\"Hello, World!\"\n",
    "        ciphertext = public_key.encrypt(\n",
    "            message,\n",
    "            padding.OAEP(mgf=padding.MGF1(algorithm=hashes.SHA256()), algorithm=hashes.SHA256(), label=None)\n",
    "        )\n",
    "        plaintext = private_key.decrypt(\n",
    "            ciphertext,\n",
    "            padding.OAEP(mgf=padding.MGF1(algorithm=hashes.SHA256()), algorithm=hashes.SHA256(), label=None)\n",
    "        )\n",
    "        mse = float(self.autoencoder.evaluate(self.image, self.image, verbose=0))\n",
    "        img2 = generate_logistic_map_image(initial_value=0.41)\n",
    "        private_key2, _, _, _ = generate_rsa_keys_from_image(img2[np.newaxis, ...], self.encoder)\n",
    "        avalanche = np.mean(np.abs(np.array(private_key.private_numbers().p) - np.array(private_key2.private_numbers().p)))\n",
    "        self.assertEqual(message, plaintext, \"Decryption failed\")\n",
    "        print(f\"Test encryption_decryption passed - MSE: {mse:.4f}, Generation Time: {gen_time:.3f}s, Avalanche Effect: {avalanche:.4f}\")\n",
    "\n",
    "    def test_latent_variation(self):\n",
    "        latent1 = self.encoder.predict(self.image, verbose=0)\n",
    "        new_image = generate_logistic_map_image(initial_value=0.41)\n",
    "        latent2 = self.encoder.predict(new_image[np.newaxis, ...], verbose=0)\n",
    "        diff = np.mean(np.abs(latent1 - latent2))\n",
    "        self.assertGreater(diff, 0.001, \"Latent variation too low\")\n",
    "        print(f\"Test latent_variation passed - Latent Variation: {diff:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоги\n",
    "\n",
    "Хаотичные данные повысили энтропию (~0.01 эффект лавины), но статичность алгоритма требует дообучения. Следующая версия добавит адаптивность."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
